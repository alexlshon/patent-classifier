{
  "metadata" : {
    "config" : {
      "dependencies" : {
        
      },
      "exclusions" : [
      ],
      "repositories" : [
      ],
      "sparkConfig" : {
        "spark.local.dir" : "~/tem",
        "spark.driver.memory" : "11g",
        "spark.master" : "local[*]",
        "spark.executor.memory" : "11g"
      }
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# eda pc\n",
        "\n",
        "\n",
        "Here will be the basic eda of the partitioned data \n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "jupyter.outputs_hidden" : true,
        "cell.metadata.exec_info" : {
          "startTs" : 1575884457916,
          "endTs" : 1575884458398
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.sql.types.LongType\n",
        "var df = spark.read.load(\"../Projects/data/partitioned_hdfs\")\n",
        "  .withColumn(\"id\",col(\"id\").cast(LongType))\n",
        "  .dropDuplicates(\"id\")\n",
        "  \n",
        "  "
      ],
      "outputs" : [
        {
          "ename" : "org.apache.spark.SparkException",
          "evalue" : "Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, DESKTOP-QD8U1N1.hsd1.il.comcast.net, executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:227)\n\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:285)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:437)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:482)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:476)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:73)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:837)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:837)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:455)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:830)\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=file:/home/alex/Projects/data/partitioned_hdfs/part-00000-843fa5c7-e020-4f54-b1fe-2ab2aa8f3699-c000.csv; isDirectory=false; length=184852416; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:282)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1425)\n\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1016)\n\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1665)\n\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1598)\n\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)\nCaused by: java.lang.RuntimeException: file:/home/alex/Projects/data/partitioned_hdfs/part-00000-843fa5c7-e020-4f54-b1fe-2ab2aa8f3699-c000.csv is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [45, 49, 57, 10]\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:524)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:505)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:499)\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:444)\n\t... 14 more\n\nDriver stacktrace:",
          "traceback" : [
          ],
          "output_type" : "error"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "jupyter.outputs_hidden" : true,
        "cell.metadata.exec_info" : {
          "startTs" : 1575495253721,
          "endTs" : 1575495419724
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "df.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+--------------------+-------+--------+----------+\n",
            "|              claims|     id|group_id|      date|\n",
            "+--------------------+-------+--------+----------+\n",
            "|The floor cleanin...|3930281|    A47L|1976-01-06|\n",
            "|The process of cl...|3930734|    G01D|1976-01-06|\n",
            "|A membrane formin...|3930886|    B01D|1976-01-06|\n",
            "|Method according ...|3930897|    H01F|1976-01-06|\n",
            "|The method of cla...|3930951|    C12N|1976-01-06|\n",
            "|A method as in cl...|3931049|    C22C|1976-01-06|\n",
            "|to 95 weight perc...|3931102|    C08K|1976-01-06|\n",
            "|A compound as cla...|3931222|    C07D|1976-01-06|\n",
            "|A compound as def...|3931227|    C07D|1976-01-06|\n",
            "|In an electrostat...|3931469|    H04R|1976-01-06|\n",
            "|An imaging device...|3931633|    H01L|1976-01-06|\n",
            "|2-(4-Methoxybenzy...|3932422|    C07D|1976-01-13|\n",
            "|The compound of c...|3932430|    C07D|1976-01-13|\n",
            "|Method as claimed...|3932460|    C07D|1976-01-13|\n",
            "|Apparatus for rap...|3932762|    H01Q|1976-01-13|\n",
            "|The circuit accor...|3932770|    H03K|1976-01-13|\n",
            "|A control system ...|3932773|    H03K|1976-01-13|\n",
            "|A device accordin...|3932854|    B65H|1976-01-13|\n",
            "|The device of cla...|3933309|    B64D|1976-01-20|\n",
            "|The composition o...|3933700|    C08G|1976-01-20|\n",
            "+--------------------+-------+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 3,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Lets look at the distribution of patent ids"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "jupyter.outputs_hidden" : true,
        "cell.metadata.exec_info" : {
          "startTs" : 1575495419726,
          "endTs" : 1575495519005
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "df.withColumn(\"id\",col(\"id\").cast(LongType))\n",
        "    .select(\"id\")\n",
        "    .summary()\n",
        "    .show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-------+------------------+\n",
            "|summary|                id|\n",
            "+-------+------------------+\n",
            "|  count|           6452014|\n",
            "|   mean| 7174195.463987524|\n",
            "| stddev|1874859.4698489187|\n",
            "|    min|           3930271|\n",
            "|    25%|           5549093|\n",
            "|    50%|           7172795|\n",
            "|    75%|           8795683|\n",
            "|    max|          10426069|\n",
            "+-------+------------------+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 5,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "The distribution of the ids makes sense, Now lets count the number of nulls in the rest of the columns \n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "jupyter.outputs_hidden" : true,
        "cell.metadata.exec_info" : {
          "startTs" : 1575495551771,
          "endTs" : 1575495552152
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "df.filter(df(\"claims\") === null).count()"
      ],
      "outputs" : [
        {
          "execution_count" : 6,
          "data" : {
            "text/plain" : [
              "0"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Long"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 7,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Now lets look at the distribution of the sub classes "
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 8,
      "metadata" : {
        "jupyter.outputs_hidden" : true,
        "cell.metadata.exec_info" : {
          "startTs" : 1575509309216,
          "endTs" : 1575509403813
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val subClasses = df.groupBy(\"group_id\").count().cache()\n",
        "subClasses.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+--------+-----+\n",
            "|group_id|count|\n",
            "+--------+-----+\n",
            "|    D06M| 3850|\n",
            "|    C05C|  337|\n",
            "|    C12H|  277|\n",
            "|    B60T|15636|\n",
            "|    B63B|12010|\n",
            "|    A01B| 4917|\n",
            "|    C23C|24419|\n",
            "|    D06G|   60|\n",
            "|    A63K|  125|\n",
            "|    E06B|11297|\n",
            "|    E03C| 3538|\n",
            "|    B27H|   41|\n",
            "|    A21C| 1940|\n",
            "|    A24F| 2516|\n",
            "|    G21G|  409|\n",
            "|    C10M| 7720|\n",
            "|    B03C| 2857|\n",
            "|    F41C| 2424|\n",
            "|    A46B| 4204|\n",
            "|    F01P| 2977|\n",
            "+--------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "subClasses.select(\"count\").summary().show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-------+------------------+\n",
            "|summary|             count|\n",
            "+-------+------------------+\n",
            "|  count|               641|\n",
            "|   mean| 10065.54446177847|\n",
            "| stddev|27660.445523882314|\n",
            "|    min|                 1|\n",
            "|    25%|               810|\n",
            "|    50%|              2826|\n",
            "|    75%|              8815|\n",
            "|    max|            404420|\n",
            "+-------+------------------+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 10,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1575509403815,
          "endTs" : 1575509405852
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val low_count = subClasses.sort(\"count\")\n",
        "low_count.show(100)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+--------+-----+\n",
            "|group_id|count|\n",
            "+--------+-----+\n",
            "|    Y02B|    1|\n",
            "|    G06D|    1|\n",
            "|    A61P|    1|\n",
            "|    B29K|    1|\n",
            "|    Y02W|    1|\n",
            "|    Y02D|    2|\n",
            "|    Y02E|    3|\n",
            "|    Y02P|    6|\n",
            "|    Y02A|    7|\n",
            "|    B68F|    8|\n",
            "|    C12L|    9|\n",
            "|    G21J|   10|\n",
            "|    C10H|   12|\n",
            "|    F21W|   13|\n",
            "|    C06F|   14|\n",
            "|    F16S|   14|\n",
            "|    G16Z|   14|\n",
            "|    E02C|   19|\n",
            "|    B41G|   20|\n",
            "|    F21H|   20|\n",
            "|    F17B|   23|\n",
            "|    B41D|   26|\n",
            "|    D01C|   28|\n",
            "|    B27J|   33|\n",
            "|    C09H|   35|\n",
            "|    G10B|   39|\n",
            "|    B27H|   41|\n",
            "|    C12J|   41|\n",
            "|    C12F|   42|\n",
            "|    D06J|   43|\n",
            "|    B62C|   48|\n",
            "|    C10F|   55|\n",
            "|    D06G|   60|\n",
            "|    B01B|   69|\n",
            "|    H05C|   78|\n",
            "|    B61J|   78|\n",
            "|    D04G|   81|\n",
            "|    Y10S|   85|\n",
            "|    C23D|   91|\n",
            "|    B31C|   99|\n",
            "|    F22G|  110|\n",
            "|    A63K|  125|\n",
            "|    B21G|  126|\n",
            "|    B82B|  127|\n",
            "|    G06C|  130|\n",
            "|    F23H|  138|\n",
            "|    F24T|  154|\n",
            "|    G04D|  155|\n",
            "|    G09D|  155|\n",
            "|    G21H|  164|\n",
            "|    C09F|  167|\n",
            "|    D01B|  179|\n",
            "|    D02H|  182|\n",
            "|    F22D|  188|\n",
            "|    F24V|  189|\n",
            "|    G12B|  191|\n",
            "|    C14B|  191|\n",
            "|    G09C|  193|\n",
            "|    B27D|  193|\n",
            "|    A42C|  200|\n",
            "|    D06Q|  205|\n",
            "|    B21L|  208|\n",
            "|    C07G|  211|\n",
            "|    B02B|  214|\n",
            "|    B68B|  219|\n",
            "|    D04D|  220|\n",
            "|    B68G|  221|\n",
            "|    D21J|  222|\n",
            "|    B41B|  225|\n",
            "|    F03C|  226|\n",
            "|    F23B|  226|\n",
            "|    F16T|  244|\n",
            "|    G10F|  251|\n",
            "|    G06J|  255|\n",
            "|    F28B|  267|\n",
            "|    C12H|  277|\n",
            "|    F03H|  278|\n",
            "|    C05D|  279|\n",
            "|    C12G|  280|\n",
            "|    D06L|  287|\n",
            "|    G04R|  293|\n",
            "|    C06C|  312|\n",
            "|    D03J|  312|\n",
            "|    B60M|  314|\n",
            "|    G16C|  314|\n",
            "|    A01L|  317|\n",
            "|    D06H|  324|\n",
            "|    C10K|  335|\n",
            "|    C05C|  337|\n",
            "|    G06E|  337|\n",
            "|    B60V|  342|\n",
            "|    C06D|  349|\n",
            "|    F15C|  353|\n",
            "|    B33Y|  361|\n",
            "|    C05B|  361|\n",
            "|    B60F|  365|\n",
            "|    B68C|  373|\n",
            "|    B63J|  380|\n",
            "|    B61H|  403|\n",
            "|    G21G|  409|\n",
            "+--------+-----+\n",
            "only showing top 100 rows\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 11,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1575509642981,
          "endTs" : 1575509644104
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "low_count.limit(100).agg(sum($\"count\")).show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|     16234|\n",
            "+----------+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 12,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "The sub classes are extremely unbalanced. The 100 least used classes are used about 16234 times \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    }
  ]
}